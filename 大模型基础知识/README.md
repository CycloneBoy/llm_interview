[toc]

# 大模型基础知识


## Transformer

### attention

Self-attention

Multi-Head Attention (MHA)

Multi-Query Attention (MQA)

Group-Query Attention (GQA)

Sliding Window Attention (SWA)

Flash Attention

Flash Attention v2

Paged Attention

### 位置编码

SinCos

rotary

Alibi

### tokenize

Byte-Pair Encoding(BPE)

SentencePiece

WordPiece

## 常见大模型

### Llama

### Qwen

### Deepseek

## 提示工程


Zero-Shot

Few-Shot Prompting

Chain-of-Thought (CoT)

Tree-of-Thoughts (ToT) Prompting

Automatic Chain-of-Thought (Auto-CoT) Prompting

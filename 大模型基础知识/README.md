# 大模型基础知识

## Transformer

### [attention](/大模型基础知识/attention.md)

Self-attention

Multi-Head Attention (MHA)

Multi-Query Attention (MQA)

Group-Query Attention (GQA)

Sliding Window Attention (SWA)

Flash Attention

Flash Attention v2

Paged Attention

多头潜在注意力机制 (MLA) （Deepseek v3）

### [位置编码](/大模型基础知识/位置编码.md)

## 1.SinCos

## 2.Alibi

## 3.SinCos

## 4.rotary

## 5.长度外推

## 6.Yarn

SinCos

rotary

Alibi

长度外推

Yarn

### tokenize

Byte-Pair Encoding(BPE)

SentencePiece

WordPiece

## 常见大模型

### Llama

### Qwen

### Deepseek

## 提示工程


Zero-Shot

Few-Shot Prompting

Chain-of-Thought (CoT)

Tree-of-Thoughts (ToT) Prompting

Automatic Chain-of-Thought (Auto-CoT) Prompting

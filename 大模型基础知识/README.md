# 大模型基础知识

## Transformer

### [attention](/大模型基础知识/attention.md)

Self-attention

Multi-Head Attention (MHA)

Multi-Query Attention (MQA)

Group-Query Attention (GQA)

Sliding Window Attention (SWA)

Flash Attention

Flash Attention v2

Paged Attention

多头潜在注意力机制 (MLA) （Deepseek v3）

### [位置编码](/大模型基础知识/positional_encoding.md)

SinCos

Rope

Alibi

长度外推

Yarn

长上下文

### tokenize

Byte-Pair Encoding(BPE)

SentencePiece

WordPiece

## 常见大模型

### Llama

### Qwen

### Deepseek

## 提示工程

Zero-Shot

Few-Shot Prompting

Chain-of-Thought (CoT)

Tree-of-Thoughts (ToT) Prompting

Automatic Chain-of-Thought (Auto-CoT) Prompting

## [混合专家模型 (MoE)](/大模型基础知识/moe.md)
